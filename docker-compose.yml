services:
  llama-server:
    build:
      context: ./llama-server
      dockerfile: Dockerfile
    container_name: llama-server
    # command: ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "5050"]
    working_dir: /app
    volumes:
      - ./llama-server:/app
    env_file:
      - .env
    environment:
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - TRANSFORMERS_VERBOSITY=info
      - TRANSFORMERS_NO_ADVISORY_WARNINGS=1
      - HF_TOKEN=${HF_TOKEN}
      - MODEL_ID=${MODEL_ID}
    ports:
      - "5050:5050"
    restart: unless-stopped
    command: ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "5050","--reload"]

  # ai-server:
  #   build:
  #     context: ./ai-server
  #     dockerfile: Dockerfile
  #   container_name: thunderbird-ai
  #   env_file:
  #     - .env
  #   depends_on:
  #     - llama-server
  #   ports:
  #     - "4000:4000"
  #   restart: unless-stopped
