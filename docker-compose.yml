services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: always
    environment:
      - OLLAMA_MODELS=/root/.ollama/models
  fastapi-server:
    build:
      context: ./fastapi-server
      dockerfile: Dockerfile
    ports:
      - "5050:5050"
    depends_on:
      - ollama
    volumes:
      - ./fastapi-server:/app
       - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
    restart: always
volumes:
  ollama_data:
    driver: local

  # llama-server:
  #   build:
  #     context: ./llama-server
  #     dockerfile: Dockerfile
  #   container_name: llama-server
  #   working_dir: /app
  #   volumes:
  #     - ./llama-server:/app
  #   env_file:
  #     - .env
  #   environment:
  #     - PYTHONUNBUFFERED=1
  #     - HF_TOKEN=${HF_TOKEN}
  #     - MODEL_ID=${MODEL_ID}
  #   ports:
  #     - "5050:5050"
  #   restart: unless-stopped
  #   command: ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "5050","--reload"]  # Remove `--reload` for production

