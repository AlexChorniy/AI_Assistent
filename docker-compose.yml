services:
  llama-server:
    platform: linux/amd64  # Force AMD64 architecture
    build:
      context: ./llama-server
      dockerfile: Dockerfile
      args:
        HF_TOKEN: ${HF_TOKEN} # Pass the HF_TOKEN as a build argument
    container_name: llama-server
    command: ["node", "server.js"]  # Running directly from server.js instead of build/server.js
    working_dir: /app
    volumes:
      - ./llama-server:/app
      - /app/node_modules  # Prevents node_modules from being overridden
    env_file:
      - .env
    ports:
      - "5050:5050"
    restart: unless-stopped

  # ai-server:
  #   build:
  #     context: ./ai-server
  #     dockerfile: Dockerfile
  #   container_name: thunderbird-ai
  #   env_file:
  #     - .env
  #   depends_on:
  #     - llama-server
  #   ports:
  #     - "4000:4000"
  #   restart: unless-stopped
