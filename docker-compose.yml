version: '3.8'

services:
  llama-server:
    platform: linux/amd64  # Force AMD64 architecture
    build:
      context: ./llama-server
      dockerfile: Dockerfile
      args:
        HF_TOKEN: ${HF_TOKEN} # Pass the HF_TOKEN as a build argument
    container_name: llama-server
    env_file:
      - .env
    ports:
      - "5050:5050"
    restart: unless-stopped

  ai-server:
    build:
      context: ./ai-server
      dockerfile: Dockerfile
    container_name: thunderbird-ai
    env_file:
      - .env
    depends_on:
      - llama-server
    ports:
      - "4000:4000"
    restart: unless-stopped
